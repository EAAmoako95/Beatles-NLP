{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0694e9b1",
   "metadata": {},
   "source": [
    "##### Caretaking*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ddbb3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371b866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e82b46",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4864b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0c4cbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langdetect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatistics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mode\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangdetect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect, detect_langs\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langdetect'"
     ]
    }
   ],
   "source": [
    "from tqdm import notebook\n",
    "from collections import Counter\n",
    "\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from itertools import combinations\n",
    "from statistics import mode\n",
    "\n",
    "from langdetect import detect, detect_langs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedfce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354fec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"C:\\\\Users\\\\ernes\\\\anaconda3\\\\Lib\\\\site-packages\\\\nltk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a3dce",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12966ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_path = './lyrics/'\n",
    "new_lyrics_path = './new_lyrics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fdbfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get folder contents first of all...\n",
    "lyrics_ls = [f for f in os.listdir(lyrics_path) if f.endswith('.txt')]\n",
    "new_lyrics_ls = [f for f in os.listdir(new_lyrics_path) if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f884a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and then read contents to two different lists.\n",
    "lyrics = [[file.replace(\".txt\",\"\"),open(os.path.join(lyrics_path, file), 'r').read()] for file in lyrics_ls]\n",
    "new_lyrics = [[file.replace(\".txt\",\"\"),open(os.path.join(new_lyrics_path, file), 'r').read()] for file in new_lyrics_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d25d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_lyrics = pd.DataFrame(lyrics,columns=['Song','Lyrics'])\n",
    "combined_new_lyrics = pd.DataFrame(new_lyrics,columns=['Song','Lyrics'])\n",
    "\n",
    "combined_lyrics['New/Old'] = \"old\"\n",
    "combined_new_lyrics['New/Old'] = \"new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a409f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lyrics = pd.concat([combined_lyrics,combined_new_lyrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a43ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lyrics['New/Old'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8844c3f",
   "metadata": {},
   "source": [
    "## Objective 1: Account for Duplicates and Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934dc11d",
   "metadata": {},
   "source": [
    "- Account for duplicates in data files:\n",
    "\t- example: \"a-day-in-the-life\" and \"a-day-in-the-life-live-in-amsterdam\"\n",
    "- \"weird\" or \"missing\" data.\n",
    "- Remove anything that looks \"amiss\"\n",
    "- Essentially data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3bcfe",
   "metadata": {},
   "source": [
    "###### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_content_check(x):\n",
    "    \n",
    "    assess = (x != '\\n' and \"instrumental\" not in x.lower())\n",
    "    \n",
    "    return assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf044fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lyrics(lys):\n",
    "    \n",
    "    rgx_pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    \n",
    "    sentences_ls = lys.split('\\n')\n",
    "    \n",
    "    std_ls = [re.sub(rgx_pattern,'',x.strip()).lower() for x in sentences_ls if len(x) > 0]\n",
    "    \n",
    "    return std_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_lyrics_adj = total_lyrics[total_lyrics['Lyrics'].apply(simple_content_check)] # adjusted version to exclude based on function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_lyrics_adj['New/Old'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to check if any adjustments or additions to function above\n",
    "print(f\"{np.round((len(lyrics) - len(combined_lyrics_adj))/len(lyrics) * 100,2)} % reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf8d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all duplicate instances on the lyrics column\n",
    "# Keep first instance by default\n",
    "combined_lyrics_adj.drop_duplicates(subset=['Lyrics'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_lyrics_adj.reset_index(drop=True,inplace=True)\n",
    "combined_lyrics_adj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d87cb3",
   "metadata": {},
   "source": [
    "###### Language Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5626e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account for any language differences\n",
    "combined_lyrics_adj['Languages'] = combined_lyrics_adj['Lyrics'].apply(lambda x: detect(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af88d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_lyrics_adj['Languages'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da340946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fault of the detection module. Funny one though!\n",
    "#combined_lyrics_adj.loc[combined_lyrics_adj['Languages'] == 'so']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#odd_idx = combined_lyrics_adj.loc[combined_lyrics_adj['Languages'] == 'so'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34841ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting. As above ^\n",
    "#detect_langs(combined_lyrics_adj['Lyrics'][odd_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two known instances of German versions of Beatles songs being released.\n",
    "# These pertain to \"I Want To Hold Your Hand\" and \"She Loves You\"\n",
    "# Will omit as the English lang versions of both are already included.\n",
    "combined_lyrics_adj.loc[combined_lyrics_adj['Languages'] == 'de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67773cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_lyrics_adj = combined_lyrics_adj.loc[combined_lyrics_adj['Languages'] != 'de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c70cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_lyrics_adj.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275391f1",
   "metadata": {},
   "source": [
    "###### Cosine Similarity of song lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647fe0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(combined_lyrics_adj['Lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac94f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_cs = [] # List for cosine similarity\n",
    "old_df = combined_lyrics_adj.loc[combined_lyrics_adj['New/Old']==\"old\"]\n",
    "rng_max = len(old_df)\n",
    "for i in notebook.tqdm(range(rng_max)):\n",
    "    for j in range(i+1,rng_max): # Avoid self-matching\n",
    "        \n",
    "        # Lyrics for each song pairing\n",
    "        lyrics_i = tfidf_matrix[i]\n",
    "        lyrics_j = tfidf_matrix[j]\n",
    "        \n",
    "        # Compute cosine similarity score\n",
    "        sim_score = cosine_similarity(lyrics_i,lyrics_j)[0][0]\n",
    "        \n",
    "        comparisons_cs.append({\n",
    "            'Song1': old_df['Song'][i],\n",
    "            'Song2': old_df['Song'][j],\n",
    "            'Similarity': sim_score\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db273da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_cs_df = pd.DataFrame(comparisons_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14057c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rationale: the general and short nature of the song \"the-end\" is causing it to compare...\n",
    "# ...favourably to many songs. Omitting to inspect further\n",
    "comp_cs_df_redux = comp_cs_df[~comp_cs_df['Song1'].str.contains('the-end', case=False) & ~comp_cs_df['Song2'].str.contains('the-end', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comfortable to take anything at .7 and above as actually being the same\n",
    "# And can inspect by eye from here.\n",
    "# Revolution has a copy with additional lyrics\n",
    "# Sgt Peppers Lonely Hearts Club Band has a reprise\n",
    "comp_cs_df_redux.sort_values('Similarity',ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e4e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_list = list(comp_cs_df_redux.loc[comp_cs_df_redux['Similarity']>=0.7]['Song1'].values)\n",
    "omit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_clean = combined_lyrics_adj[combined_lyrics_adj['Song'].apply(lambda x: x not in omit_list)]\n",
    "lyrics_clean.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8e9a6",
   "metadata": {},
   "source": [
    "# Objective 2: Answer 2 of 5 Available Questions of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396e7b0",
   "metadata": {},
   "source": [
    "Question 1: Which song has the largest amount of repetition?\n",
    "\n",
    "Question 2: How many of the songs feature the song name (found in the file name) in the song lyrics?\n",
    "\n",
    "Bonus (from above data cleaning): Which songs are the most similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d008b37",
   "metadata": {},
   "source": [
    "### Which song has the largest amount of repetition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a2c2e",
   "metadata": {},
   "source": [
    "- Taking the definition of repetition as all the unique lines in  a song that occur more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496f00b",
   "metadata": {},
   "source": [
    "###### Basic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260eaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetition_counter(lys):\n",
    "    \n",
    "    standardised_ls = clean_lyrics(lys)\n",
    "    \n",
    "    standardised_srs = pd.Series(standardised_ls)\n",
    "    instance_count = standardised_srs.value_counts()\n",
    "    \n",
    "    # Terminology may be off here, but count the length of all instances greater than 1\n",
    "    # Call that number of instances\n",
    "    # Then the frequency of repetitions in a set of lyrics is the sum of the count of instances\n",
    "    \n",
    "    repetition_instances = len(instance_count[instance_count > 1]) # How many lines repeat\n",
    "    repetition_frequency = instance_count[instance_count > 1].sum() # What is the sum of those repetitions\n",
    "    \n",
    "    return [repetition_instances,repetition_frequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_clean['Results'] = lyrics_clean['Lyrics'].apply(repetition_counter)\n",
    "lyrics_clean[['Repetition Instances','Repetition Frequency']] = lyrics_clean['Results'].apply(pd.Series)\n",
    "lyrics_clean.drop('Results',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723c100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_clean.head()#.sort_values('Repetition Frequency',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa890a",
   "metadata": {},
   "source": [
    "###### N-gram Similarity - More Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064b1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_sim(lys,jcrd_thresh=1.0,inspect=False):\n",
    "    \n",
    "    standardised_ls = clean_lyrics(lys)\n",
    "    \n",
    "    # Tokenize lyrics and run calculation\n",
    "    token_lyrics = [nltk.word_tokenize(x.lower()) for x in standardised_ls]\n",
    "    n_grams = [list(ngrams(y,2)) for y in token_lyrics]\n",
    "    \n",
    "    #return n_grams\n",
    "    jcrd_similarity = []\n",
    "    for i in range(len(standardised_ls)):\n",
    "        for j in range(i + 1, len(standardised_ls)): # Again, avoid self-comparison\n",
    "            try:\n",
    "                similarity = 1 - jaccard_distance(set(n_grams[i]), set(n_grams[j]))\n",
    "                if similarity >= jcrd_thresh: # If they match based on the set threshold then they can be included\n",
    "                    jcrd_similarity.append([n_grams[i],n_grams[j],similarity])\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    # For inspection and testing purposes...or not.\n",
    "    if inspect:\n",
    "        return jcrd_similarity\n",
    "    else:\n",
    "        return len(jcrd_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_clean['N_Gram_Similarity'] = lyrics_clean['Lyrics'].apply(n_gram_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rankings as seen below. Preliminary check of the top few confirms.\n",
    "lyrics_clean.sort_values('N_Gram_Similarity',ascending=False)[['Song','N_Gram_Similarity']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff811d",
   "metadata": {},
   "source": [
    "### How many of the songs feature the song name (found in the file name) in the song lyrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d765875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_in_lyrics(row):\n",
    "    \n",
    "    clean_song = row['Song'].split('-live')[0].replace(\"-\",\" \")\n",
    "    \n",
    "    standardised_ls = clean_lyrics(row['Lyrics'])\n",
    "    \n",
    "    return clean_song in standardised_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_clean['Name_In_Lyrics'] = lyrics_clean[['Song','Lyrics']].apply(song_in_lyrics, axis=1)\n",
    "lyrics_clean['Name_In_Lyrics'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d353ac1c",
   "metadata": {},
   "source": [
    "# Objective 3: Derive a single piece of insight from the data that you find interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd559e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "btls_context_data = pd.read_csv('./Context Data/The Beatles songs dataset, v1, no NAs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corresponding \"Song\" column for this df to match the main df.\n",
    "hyphen_pattern = r'[^\\w\\s-]'\n",
    "btls_context_data['Song'] = btls_context_data['Title'].apply(lambda x: re.sub(hyphen_pattern,'',x).replace(' ','-').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference list. Of a negligible enough number that I'm happy to continue.\n",
    "diff_ls = list(set(lyrics_clean['Song'].apply(lambda x: x.lower())) - set(btls_context_data['Song']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0254328",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = lyrics_clean.loc[~lyrics_clean['Song'].isin(diff_ls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df['Song'] = diff_df['Song'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e97df",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_df = pd.merge(diff_df,btls_context_data[['Song',\n",
    "                                                 'Year',\n",
    "                                                 'Duration',\n",
    "                                                 'Genre',\n",
    "                                                 'Songwriter',\n",
    "                                                 'Lead.vocal',\n",
    "                                                 'Top.50.Billboard']],on='Song')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c686f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minor negative correlation (aka, the higher the level of repetition, the higher it was on the charts)\n",
    "subset_1 = context_df.copy().loc[context_df[\"Top.50.Billboard\"] != -1] # only instances where it came top 50 billboard charts\n",
    "subset_1['N_Gram_Similarity'].corr(subset_1['Top.50.Billboard']) # simple pearson corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33744d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By injecting context, we can see what the average n_gram_similarity is by lead singer\n",
    "# With the n_gram_similarity (bigram if we look back at the parameters set when the value was calculated)...\n",
    "# ... being our measure for repetition.\n",
    "context_col = \"Lead.vocal\"\n",
    "context_df[[context_col,'N_Gram_Similarity']].groupby(context_col).mean()[['N_Gram_Similarity']].sort_values('N_Gram_Similarity',ascending=False).iplot(kind='bar',title=f\"Average Repetition by {context_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b0827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mode(column):\n",
    "    try:\n",
    "        return mode(column)\n",
    "    except StatisticsError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, but only looking at instances where they made it to the billboard top 50\n",
    "# Features average chart position.\n",
    "\n",
    "aggregations = {\n",
    "    'N_Gram_Similarity': 'mean',  # Sum of column A\n",
    "    'Top.50.Billboard': calculate_mode,  # Mean of column B\n",
    "}\n",
    "\n",
    "context_col = \"Lead.vocal\"\n",
    "subset_1[[context_col,'N_Gram_Similarity','Top.50.Billboard']].groupby(context_col).agg(aggregations)[['N_Gram_Similarity',\"Top.50.Billboard\"]].sort_values('N_Gram_Similarity',ascending=False).iplot(kind='bar',title=f\"Average Repetition by {context_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2759a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_a = \"Duration\"\n",
    "col_b = \"N_Gram_Similarity\"\n",
    "correlation = context_df[col_a].corr(context_df[col_b])\n",
    "context_df[['Year','Duration','N_Gram_Similarity']].groupby('Year').mean().iplot(title=f'Average {col_a} and {col_b} per Year - correlation: {np.round(correlation,3)}',\n",
    "                                                                                 xTitle='Year',\n",
    "                                                                                 yTitle='Average X')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e1ebf",
   "metadata": {},
   "source": [
    "# Objective 4&5: Put Songs into Clusters - Cluster Remaining 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45fdd0d",
   "metadata": {},
   "source": [
    "- First adding useful context before clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143f729",
   "metadata": {},
   "source": [
    "###### Sentiment Analysis of Topics and Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lyric = lyrics_clean['Lyrics'][10] # One set of lyrics as a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf23a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer() # Init SIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(test_lyric)['compound'] # Quick test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_assignment(score):\n",
    "    # Custom sentiment categories and threshold ranges\n",
    "    # Covering regular intervals from 1 to -1\n",
    "    sentiment_categories = {\n",
    "        \"Strongly Positive\": (0.5, 1.0),\n",
    "        \"Moderately Positive\": (0.2, 0.5),\n",
    "        \"Slightly Positive\": (0.05, 0.2),\n",
    "        \"Neutral\": (-0.05, 0.05),\n",
    "        \"Slightly Negative\": (-0.2, -0.05),\n",
    "        \"Moderately Negative\": (-0.5, -0.2),\n",
    "        \"Strongly Negative\": (-1.0, -0.5),\n",
    "    }\n",
    "\n",
    "    # Sentiment category based on the custom thresholds\n",
    "    for category, (lower_threshold, upper_threshold) in sentiment_categories.items():\n",
    "        if lower_threshold <= score <= upper_threshold:\n",
    "            return category\n",
    "    \n",
    "    return \"Undefined\"  # Default category if the score is outside of defined thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_assignment(1) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_clean['Compound Sentiment Score'] = lyrics_clean['Lyrics'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "lyrics_clean['Compound Sentiment Assignment'] = lyrics_clean['Compound Sentiment Score'].apply(lambda x: compound_assignment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f59634",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(lyrics_clean['Lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff18473",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Lyrics','N_Gram_Similarity', \n",
    "                'Compound Sentiment Score',\n",
    "                \"Repetition Instances\", \n",
    "                \"Repetition Frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54463228",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = lyrics_clean[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_tfidf = tfidf_vectorizer.fit_transform(feature_df['Lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_tfidf_df = pd.DataFrame(lyrics_tfidf.toarray(),\n",
    "                               columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = pd.concat([lyrics_tfidf_df,feature_df.drop('Lyrics',axis=1)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97392cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Elbow Method\n",
    "range_upper_bound = int(len(lyrics_clean['Lyrics'].unique())/4) \n",
    "range_n_clusters = list(range(1,range_upper_bound))\n",
    "\n",
    "sum_squares = []\n",
    "for n_clusters in notebook.tqdm(range_n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(feature_matrix)\n",
    "    sum_squares.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241aeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(sum_squares).iplot(title='Elbow Plot',\n",
    "                             xTitle='Clusters',\n",
    "                             yTitle='SSDs') # I call it about 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055336bb",
   "metadata": {},
   "source": [
    "###### Kneedle Locate value of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95463427",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_df = pd.DataFrame(sum_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kneedle = KneeLocator(elbow_df.index[1:],elbow_df[0][1:], curve=\"convex\", direction=\"decreasing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ecced",
   "metadata": {},
   "outputs": [],
   "source": [
    "kneedle.plot_knee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d95fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "kneedle.knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_cluster_num = kneedle.knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1dab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=optimal_cluster_num, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f713800",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cluster_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dddcf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_clean['Cluster Labels'] = pd.Series(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906d329",
   "metadata": {},
   "source": [
    "# Obj 4&5 pt.2 - Clustering on JUST Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8280134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new matrix based on instance of tfidf_vectorizer above.\n",
    "lyrics_clean_matrix = tfidf_vectorizer.fit_transform(lyrics_clean['Lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47410f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=optimal_cluster_num, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(lyrics_clean_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_clean['Cluster Labels LYRICS_ONLY'] = pd.Series(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2778e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a5ea228",
   "metadata": {},
   "source": [
    "### Meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f25b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_txt(text):\n",
    "    \n",
    "    # Tokenize input text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Clean and lower case it all\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_lyrs = lyrics_clean.groupby('Cluster Labels LYRICS_ONLY')\n",
    "theme_ls = []\n",
    "top_x = 20\n",
    "for cluster_label, group in grouped_lyrs:\n",
    "    \n",
    "    # Prepare corpus of words\n",
    "    all_lyrics = \" \".join(group['Lyrics'])\n",
    "    preprocessed_lyrics = preprocess_txt(all_lyrics)\n",
    "    \n",
    "    # Calc the word frequencies\n",
    "    word_frequencies = Counter(preprocessed_lyrics)\n",
    "    \n",
    "    # Get just the top 10\n",
    "    common_words = word_frequencies.most_common(top_x)\n",
    "    \n",
    "    # make dataframe output\n",
    "    common_words_df = pd.DataFrame(common_words, columns=['Word', 'Frequency'])\n",
    "    \n",
    "    # Add a new column for the cluster label\n",
    "    common_words_df['Cluster Label'] = cluster_label\n",
    "    \n",
    "    # Add the DataFrame to the list\n",
    "    theme_ls.append(common_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e08b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f672d50e",
   "metadata": {},
   "source": [
    "# Sandbox and Outputs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f323d7b0",
   "metadata": {},
   "source": [
    "# Cluster distribution based on tfidf matrix combined df\n",
    "pd.DataFrame(lyrics_clean[\"Cluster Labels\"].value_counts()).sort_index()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5578351",
   "metadata": {},
   "source": [
    "# Cluster distribution based only on lyrics turned into sparse matrix\n",
    "# via tfidf\n",
    "pd.DataFrame(lyrics_clean[\"Cluster Labels LYRICS_ONLY\"].value_counts()).sort_index()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11ba7ba9",
   "metadata": {},
   "source": [
    "lyrics_clean.sort_values('N_Gram_Similarity',ascending=False)[['Song','N_Gram_Similarity']].to_csv('Repetition Dataset.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5776c64c",
   "metadata": {},
   "source": [
    "comp_cs_df_redux.sort_values('Similarity',ascending=False)[2:]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eff45827",
   "metadata": {},
   "source": [
    "context_df[[context_col,'N_Gram_Similarity']].groupby(context_col).mean()[['N_Gram_Similarity']].sort_values('N_Gram_Similarity',ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91af02dc",
   "metadata": {},
   "source": [
    "context_col = \"Lead.vocal\"\n",
    "subset_1[[context_col,'N_Gram_Similarity','Top.50.Billboard']].groupby(context_col).agg(aggregations)[['N_Gram_Similarity',\"Top.50.Billboard\"]].sort_values('N_Gram_Similarity',ascending=False).iplot(kind='bar',title=f\"Average Repetition by {context_col}\",asUrl=True,filename=\"Average Repetition by Lead Vocalist (Billboard Charts)\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95e4a6de",
   "metadata": {},
   "source": [
    "cluster_meanings = {}\n",
    "for cluster in range(optimal_cluster_num):\n",
    "    cluster_songs = lyrics_clean[lyrics_clean['Cluster Labels LYRICS_ONLY'] == cluster]['Song'].tolist()\n",
    "    cluster_meanings[f'Cluster {cluster+1}'] = cluster_songs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1455e57e",
   "metadata": {},
   "source": [
    "cluster_result_df = pd.DataFrame([pd.Series(x) for x in cluster_meanings.values()]).T\n",
    "cluster_result_df.columns = cluster_meanings.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aee9a171",
   "metadata": {},
   "source": [
    "cluster_result_df.to_csv('Cluster Allocation LYRICS ONLY.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0739c21c",
   "metadata": {},
   "source": [
    "cluster_result_df.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4eb96794",
   "metadata": {},
   "source": [
    "\"Song\",\"Lyrics\",\"Cluster Labels LYRICS_ONLY\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "43efebd3",
   "metadata": {},
   "source": [
    "lyrics_clean"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09f9fb9f",
   "metadata": {},
   "source": [
    "nltk.data.path.append(\"C:\\\\Users\\\\ernes\\\\Documents\\\\Python Scripts\\\\manual_dloads\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "697dfe32",
   "metadata": {},
   "source": [
    "# Check if the custom stopwords resource is found\n",
    "if nltk.data.find('corpora/stopwords/english'):\n",
    "    print(\"Custom stopwords resource found.\")\n",
    "else:\n",
    "    print(\"Custom stopwords resource not found.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1165fbc1",
   "metadata": {},
   "source": [
    "[df.to_csv(f\"Cluster_{df['Cluster Label'].unique()[0]}_Themes.csv\") for df in theme_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35408b7b-6223-413d-aad9-163fade0321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('fasten.v.01') cause to be firmly attached\n",
      "Synset('hook.n.04') a mechanical device that is curved or bent to suspend or hold or pull something\n",
      "Synset('support.n.03') something providing immaterial assistance to a person or cause or interest\n",
      "Synset('television_reporter.n.01') someone who reports news stories via television\n",
      "Synset('commissioned_military_officer.n.01') a commissioned officer in the Army or Air Force or Marine Corps\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Ensure you have the WordNet data downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # For extended synsets\n",
    "\n",
    "words = [\"anchor\", \"captain\", \"deck\", \"rudder\"]\n",
    "\n",
    "# Get synsets for each word\n",
    "synsets = [wn.synsets(word) for word in words]\n",
    "\n",
    "# Extract the most common hypernyms (general concepts)\n",
    "hypernyms = []\n",
    "for synset_list in synsets:\n",
    "    for synset in synset_list:\n",
    "        hypernyms.extend(synset.hypernyms())\n",
    "\n",
    "# Count the frequency of each hypernym\n",
    "from collections import Counter\n",
    "hypernym_counts = Counter(hypernyms)\n",
    "\n",
    "# Get the most common hypernym\n",
    "most_common_hypernyms = hypernym_counts.most_common(5)\n",
    "for hypernym, count in most_common_hypernyms:\n",
    "    print(hypernym, hypernym.definition())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93dbb0e-9c93-45ce-8a63-78c26193416a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
